{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GLG6fYd8oTh6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "16UlZVwO6-Sl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=tfds.load('judge', as supervised=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "d19ff78a-83a7-4b6a-b810-5828c17476ac",
        "id": "xU2INh8E-e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-16-1c9fc4cf33b0>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-1c9fc4cf33b0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    dataset=tfds.load('judge', as supervised=True)\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset,test_datase=dataset['train'],dataset['test']"
      ],
      "metadata": {
        "id": "voSXiXHQ6fI7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch size=32\n",
        "train_dataset=train_dataset.shuffle(10000)\n",
        "train_dataset=train_dataset.batch(batch_size)\n",
        "test_dataset=test_dataset.batch(batch_size)\n"
      ],
      "metadata": {
        "id": "Ce_6fe7H-aen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder=tf.keras.layersTextVectorization()\n",
        "encoder.adapt(train_dataset.map(lamda text,_:text))"
      ],
      "metadata": {
        "id": "-2KOQXqyoWw8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary=np.array(encoder.get_vocabulary())"
      ],
      "metadata": {
        "id": "K_DuYcDJolbr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_text=np.array(train_dataset.as_numpy_iterator())[0][0]\n",
        "encoded_text=encoder(original_text)\n",
        "decoded_text=''.join(vocabulary[encoded_text])"
      ],
      "metadata": {
        "id": "YJKDkvIg-Hph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FafxIEFPBfSi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}